<!DOCTYPE html><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>1 Subset selection, Regularization and Dimension Reduction</title>
<!--Generated on Mon Dec 15 22:42:48 2025 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<link rel="stylesheet" href="ltx-listings.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
</head>
<body>
<nav class="toc" aria-label="Table of contents">
<h2>Contents</h2>
<ul>
<li><a href="#sec-1-subset-selection-regularization-and-dimension-reduction">1 Subset selection, Regularization and Dimension Reduction</a></li>
<ul>
<li><a href="#subsec-11-subset-selection">1.1 Subset Selection</a></li>
<ul>
<li><a href="#subsubsec-111-best-subset-selection">1.1.1 Best Subset Selection</a></li>
<li><a href="#subsubsec-112-stepwise-selection">1.1.2 Stepwise Selection</a></li>
<li><a href="#subsubsec-113-choosing-the-optimal-model">1.1.3 Choosing the Optimal Model</a></li>
</ul>
<li><a href="#subsec-12-shrinkage-methods">1.2 Shrinkage Methods</a></li>
<ul>
<li><a href="#subsubsec-121-ridge-regression">1.2.1 Ridge Regression</a></li>
<li><a href="#subsubsec-122-the-lasso">1.2.2 The Lasso</a></li>
<li><a href="#subsubsec-123-selecting-the-tuning-parameter-for-ridge-regression-and-lasso">1.2.3 Selecting the Tuning Parameter for Ridge Regression and Lasso</a></li>
</ul>
<li><a href="#subsec-13-dimension-reduction-methods">1.3 Dimension Reduction Methods</a></li>
<ul>
<li><a href="#subsubsec-131-principal-components-regression">1.3.1 Principal Components Regression</a></li>
<li><a href="#subsubsec-132-partial-least-squares">1.3.2 Partial Least Squares</a></li>
</ul>
</ul>
</ul></nav>

<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" id="sec-1-subset-selection-regularization-and-dimension-reduction">
<span class="ltx_tag ltx_tag_section">1 </span>Subset selection, Regularization and Dimension Reduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">We consider some approaches for extending the linear model framework: we generalize the linear model in order to accommodate non-linear, but still additive, relationships.

<br class="ltx_break">Despite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance. We’ll discuss some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures. <em class="ltx_emph ltx_font_italic">Why consider alternatives to least squares?</em></p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">Prediction Accuracy</em>: especially when <math id="S1.I1.i1.p1.m1" class="ltx_Math" alttext="p&gt;n" display="inline"><mrow><mi>p</mi><mo>&gt;</mo><mi>n</mi></mrow></math>, to control the variance.</p>
<ul id="S1.I1.i1.I1" class="ltx_itemize">
<li id="S1.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.i1.I1.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I1.i1.I1.i1.p1" class="ltx_para">
<p class="ltx_p">in the context of linear regression, when the number of predictors is larger than the number of observations, the least squares estimation may lead to <em class="ltx_emph ltx_font_italic">overfitting</em></p>
</div>
</li>
</ul>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">Model Interpretability</em>: by removing irrelevant features — that is, by setting the corresponding coefficient estimates to zero — we can obtain a model that is more easily interpreted.</p>
</div>
</li>
</ul>
<p class="ltx_p">We present some approaches for automatically performing feature selection.
Each class of methods serves a distinct purpose in regularization, offering different ways to address the trade-off between model complexity and overfitting.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Three classes of methods</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Subset Selection</span> explicitly chooses a subset of features; we identify a subset of the <math id="S1.I2.i1.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Shrinkage</span>: reduce the impact of features; we fit a model involving all <math id="S1.I2.i2.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as <em class="ltx_emph ltx_font_italic">regularization</em>) has the effect of reducing variance and can also perform variable selection.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Dimension Reduction</span> techniques transform the feature space to reduce dimensionality; we project the <math id="S1.I2.i3.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors into a <math id="S1.I2.i3.p1.m2" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>-dimensional subspace, where <math id="S1.I2.i3.p1.m3" class="ltx_Math" alttext="M&lt;p" display="inline"><mrow><mi>M</mi><mo>&lt;</mo><mi>p</mi></mrow></math>. This is achieved by computing <math id="S1.I2.i3.p1.m4" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> different linear combinations, or projections, of the variables. Then these <math id="S1.I2.i3.p1.m5" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> projections are used as predictors to fit a linear regression model by least squares.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" id="subsec-11-subset-selection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Subset Selection</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p class="ltx_p">We present <em class="ltx_emph ltx_font_italic">best subset</em> and <em class="ltx_emph ltx_font_italic">stepwise model</em> selection procedures</p>
</div>
<section id="S1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" id="subsubsec-111-best-subset-selection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.1 </span>Best Subset Selection</h4>

<div id="S1.SS1.SSS1.p1" class="ltx_para">
<ol id="S1.I3" class="ltx_enumerate">
<li id="S1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I3.i1.p1" class="ltx_para">
<p class="ltx_p">Let <math id="S1.I3.i1.p1.m1" class="ltx_Math" alttext="M_{0}" display="inline"><msub><mi>M</mi><mn>0</mn></msub></math> denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.</p>
</div>
</li>
<li id="S1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I3.i2.p1" class="ltx_para">
<p class="ltx_p">For <math id="S1.I3.i2.p1.m1" class="ltx_Math" alttext="k=1,2,\ldots,p" display="inline"><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>p</mi></mrow></mrow></math>:</p>
<ol id="S1.I3.i2.I1" class="ltx_enumerate">
<li id="S1.I3.i2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.1</span> 
<div id="S1.I3.i2.I1.ix1.p1" class="ltx_para">
<p class="ltx_p">Fit all <math id="S1.I3.i2.I1.ix1.p1.m1" class="ltx_Math" alttext="\binom{p}{k}" display="inline"><mrow><mo>(</mo><mfrac linethickness="0pt"><mi>p</mi><mi>k</mi></mfrac><mo>)</mo></mrow></math> models that contain exactly <math id="S1.I3.i2.I1.ix1.p1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> predictors.</p>
<ul id="S1.I3.i2.I0.ix1.I1" class="ltx_itemize">
<li id="S1.I3.i2.I0.ix1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I3.i2.I0.ix1.I1.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I3.i2.I0.ix1.I1.i1.p1" class="ltx_para">
<p class="ltx_p">The binomial coefficient <math id="S1.I3.i2.I0.ix1.I1.i1.p1.m1" class="ltx_Math" alttext="\binom{p}{k}" display="inline"><mrow><mo>(</mo><mfrac linethickness="0pt"><mi>p</mi><mi>k</mi></mfrac><mo>)</mo></mrow></math> is a mathematical formula that calculates the <em class="ltx_emph ltx_font_italic">number of ways to choose <math id="S1.I3.i2.I0.ix1.I1.i1.p1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> distinct elements</em> from a <em class="ltx_emph ltx_font_italic">set of <math id="S1.I3.i2.I0.ix1.I1.i1.p1.m3" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> elements</em>. It is read as ”p choose k” and is calculated as follows:</p>
<table id="S1.Ex1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex1.m1" class="ltx_Math" alttext="\binom{p}{k}=\frac{p!}{k!(p-k)!}" display="block"><mrow><mrow><mo>(</mo><mfrac linethickness="0pt"><mi>p</mi><mi>k</mi></mfrac><mo>)</mo></mrow><mo>=</mo><mfrac><mrow><mi>p</mi><mo>!</mo></mrow><mrow><mrow><mi>k</mi><mo>!</mo></mrow><mo>⁢</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo>-</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li id="S1.I3.i2.I0.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.2</span> 
<div id="S1.I3.i2.I0.ix2.p1" class="ltx_para">
<p class="ltx_p">Pick the best among these <math id="S1.I3.i2.I0.ix2.p1.m1" class="ltx_Math" alttext="\binom{p}{k}" display="inline"><mrow><mo>(</mo><mfrac linethickness="0pt"><mi>p</mi><mi>k</mi></mfrac><mo>)</mo></mrow></math> models, and call it <math id="S1.I3.i2.I0.ix2.p1.m2" class="ltx_Math" alttext="M_{k}" display="inline"><msub><mi>M</mi><mi>k</mi></msub></math>. Here, best is defined as having the smallest RSS, or equivalently largest <math id="S1.I3.i2.I0.ix2.p1.m3" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I3.i3.p1" class="ltx_para">
<p class="ltx_p">Select a single best model from among <math id="S1.I3.i3.p1.m1" class="ltx_Math" alttext="M_{0},M_{1},\ldots,M_{p}" display="inline"><mrow><msub><mi>M</mi><mn>0</mn></msub><mo>,</mo><msub><mi>M</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>M</mi><mi>p</mi></msub></mrow></math> using cross-validated prediction error, <math id="S1.I3.i3.p1.m2" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math> (AIC), BIC, or adjusted <math id="S1.I3.i3.p1.m3" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>.</p>
</div>
</li>
</ol>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="images/fig1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering" width="379" height="178" alt="For each possible model containing a subset of the ten predictors in the Credit data set, the RSS and ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>For each possible model containing a subset of the ten predictors in the Credit data set, the RSS and <math id="S1.F1.m3" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> are displayed. The red frontier tracks the best model for a given number of predictors, according to RSS and <math id="S1.F1.m4" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>.</figcaption>
</figure>
<div id="S1.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Although we have discussed best subset selection in the context of least squares regression, the same principles can be applied to other types of models, such as <em class="ltx_emph ltx_font_italic">logistic regression</em>. In these cases, the deviance, which is negative two times the maximized log-likelihood, serves a similar role to the residual sum of squares (RSS) in a wider range of models.</p>
</div>
</section>
<section id="S1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" id="subsubsec-112-stepwise-selection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.2 </span>Stepwise Selection</h4>

<div id="S1.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">Best subset selection</em> may also suffer from statistical problems when <math id="S1.SS1.SSS2.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data. Thus an enormous search space can lead to overfitting and high variance of the coefficient estimates.

<br class="ltx_break">For both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection.</p>
</div>
<section id="S1.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Forward Stepwise Selection</h5>

<div id="S1.SS1.SSS2.Px1.p1" class="ltx_para">
<p class="ltx_p">begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.</p>
</div>
<div id="S1.SS1.SSS2.Px1.p2" class="ltx_para">
<ol id="S1.I4" class="ltx_enumerate">
<li id="S1.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I4.i1.p1" class="ltx_para">
<p class="ltx_p">Let <math id="S1.I4.i1.p1.m1" class="ltx_Math" alttext="M_{0}" display="inline"><msub><mi>M</mi><mn>0</mn></msub></math> denote the null model, which contains no predictors.</p>
</div>
</li>
<li id="S1.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I4.i2.p1" class="ltx_para">
<p class="ltx_p">For <math id="S1.I4.i2.p1.m1" class="ltx_Math" alttext="k=0,1,\ldots,p-1" display="inline"><mrow><mi>k</mi><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></mrow></mrow></math>:</p>
<ol id="S1.I4.i2.I1" class="ltx_enumerate">
<li id="S1.I4.i2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.1</span> 
<div id="S1.I4.i2.I1.ix1.p1" class="ltx_para">
<p class="ltx_p">Consider all <math id="S1.I4.i2.I1.ix1.p1.m1" class="ltx_Math" alttext="p-k" display="inline"><mrow><mi>p</mi><mo>-</mo><mi>k</mi></mrow></math> models that augment the predictors in <math id="S1.I4.i2.I1.ix1.p1.m2" class="ltx_Math" alttext="M_{k}" display="inline"><msub><mi>M</mi><mi>k</mi></msub></math> with one additional predictor.</p>
</div>
</li>
<li id="S1.I4.i2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.2</span> 
<div id="S1.I4.i2.I1.ix2.p1" class="ltx_para">
<p class="ltx_p">Choose the best among these <math id="S1.I4.i2.I1.ix2.p1.m1" class="ltx_Math" alttext="p-k" display="inline"><mrow><mi>p</mi><mo>-</mo><mi>k</mi></mrow></math> models, and call it <math id="S1.I4.i2.I1.ix2.p1.m2" class="ltx_Math" alttext="M_{k+1}" display="inline"><msub><mi>M</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></math>. Here, best is defined as having the smallest RSS or highest <math id="S1.I4.i2.I1.ix2.p1.m3" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S1.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I4.i3.p1" class="ltx_para">
<p class="ltx_p">Select a single best model from among <math id="S1.I4.i3.p1.m1" class="ltx_Math" alttext="M_{0},M_{1},\ldots,M_{p}" display="inline"><mrow><msub><mi>M</mi><mn>0</mn></msub><mo>,</mo><msub><mi>M</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>M</mi><mi>p</mi></msub></mrow></math> using cross-validated prediction error, <math id="S1.I4.i3.p1.m2" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math> (AIC), BIC, or adjusted <math id="S1.I4.i3.p1.m3" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>.</p>
</div>
</li>
</ol>
<p class="ltx_p">It is not guaranteed to find the best possible model out of all <math id="S1.SS1.SSS2.Px1.p2.m1" class="ltx_Math" alttext="2^{p}" display="inline"><msup><mn>2</mn><mi>p</mi></msup></math> models containing subsets of the <math id="S1.SS1.SSS2.Px1.p2.m2" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Variables</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Best subset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Forward stepwise</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">One</th>
<td class="ltx_td ltx_align_left ltx_border_t">rating</td>
<td class="ltx_td ltx_align_left ltx_border_t">rating</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Two</th>
<td class="ltx_td ltx_align_left">rating, income</td>
<td class="ltx_td ltx_align_left">rating, income</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Three</th>
<td class="ltx_td ltx_align_left">rating, income, student</td>
<td class="ltx_td ltx_align_left">rating, income, student</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Four</th>
<td class="ltx_td ltx_align_left ltx_border_b">cards, income, student, limit</td>
<td class="ltx_td ltx_align_left ltx_border_b">rating, income, student, limit</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The first four selected models for best subset selection and forward stepwise selection on the Credit data set. The first three models are identical but the fourth models differ.</figcaption>
</figure>
</section>
<section id="S1.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Backward Stepwise Selection</h5>

<div id="S1.SS1.SSS2.Px2.p1" class="ltx_para">
<p class="ltx_p">like forward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all <math id="S1.SS1.SSS2.Px2.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors, and then iteratively removes the least useful predictor, one-at-a-time.</p>
<ol id="S1.I5" class="ltx_enumerate">
<li id="S1.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I5.i1.p1" class="ltx_para">
<p class="ltx_p">Let <math id="S1.I5.i1.p1.m1" class="ltx_Math" alttext="M_{p}" display="inline"><msub><mi>M</mi><mi>p</mi></msub></math> denote the full model, which contains all <math id="S1.I5.i1.p1.m2" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors.</p>
</div>
</li>
<li id="S1.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I5.i2.p1" class="ltx_para">
<p class="ltx_p">For <math id="S1.I5.i2.p1.m1" class="ltx_Math" alttext="k=p,p-1,\ldots,1" display="inline"><mrow><mi>k</mi><mo>=</mo><mrow><mi>p</mi><mo>,</mo><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>1</mn></mrow></mrow></math>:</p>
<ol id="S1.I5.i2.I1" class="ltx_enumerate">
<li id="S1.I5.i2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.1</span> 
<div id="S1.I5.i2.I1.ix1.p1" class="ltx_para">
<p class="ltx_p">Consider all <math id="S1.I5.i2.I1.ix1.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> models that contain all but one of the predictors in <math id="S1.I5.i2.I1.ix1.p1.m2" class="ltx_Math" alttext="M_{k}" display="inline"><msub><mi>M</mi><mi>k</mi></msub></math>, for a total of <math id="S1.I5.i2.I1.ix1.p1.m3" class="ltx_Math" alttext="k-1" display="inline"><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></math> predictors.</p>
</div>
</li>
<li id="S1.I5.i2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.2</span> 
<div id="S1.I5.i2.I1.ix2.p1" class="ltx_para">
<p class="ltx_p">Choose the best among these <math id="S1.I5.i2.I1.ix2.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> models, and call it <math id="S1.I5.i2.I1.ix2.p1.m2" class="ltx_Math" alttext="M_{k-1}" display="inline"><msub><mi>M</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></math>. Here, best is defined as having the smallest RSS or highest <math id="S1.I5.i2.I1.ix2.p1.m3" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S1.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I5.i3.p1" class="ltx_para">
<p class="ltx_p">Select a single best model from among <math id="S1.I5.i3.p1.m1" class="ltx_Math" alttext="M_{0},M_{1},\ldots,M_{p}" display="inline"><mrow><msub><mi>M</mi><mn>0</mn></msub><mo>,</mo><msub><mi>M</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>M</mi><mi>p</mi></msub></mrow></math> using cross-validated prediction error, <math id="S1.I5.i3.p1.m2" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math> (AIC), BIC, or adjusted <math id="S1.I5.i3.p1.m3" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>.</p>
</div>
</li>
</ol>
</div>
<div id="S1.SS1.SSS2.Px2.p2" class="ltx_para">
<p class="ltx_p">Like forward stepwise selection, the backward selection approach searches through only <math id="S1.SS1.SSS2.Px2.p2.m1" class="ltx_Math" alttext="1+p(p+1)/2" display="inline"><mrow><mn>1</mn><mo>+</mo><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><mn>2</mn></mrow></mrow></math> models, and so can be applied in settings where <math id="S1.SS1.SSS2.Px2.p2.m2" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> is too large to apply best subset selection.
Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the <math id="S1.SS1.SSS2.Px2.p2.m3" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors.
Backward selection requires that the number of samples <math id="S1.SS1.SSS2.Px2.p2.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> is larger than the number of variables <math id="S1.SS1.SSS2.Px2.p2.m5" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> (so that the full model can be fit). In contrast, forward stepwise can be used even when <math id="S1.SS1.SSS2.Px2.p2.m6" class="ltx_Math" alttext="n&lt;p" display="inline"><mrow><mi>n</mi><mo>&lt;</mo><mi>p</mi></mrow></math>, and so is the only viable subset method when <math id="S1.SS1.SSS2.Px2.p2.m7" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> is very large.</p>
<ul id="S1.I6" class="ltx_itemize">
<li id="S1.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I6.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I6.i1.p1" class="ltx_para">
<p class="ltx_p">If the number of samples is less than the number of predictors, the statistical test for predictor significance may not be reliable, as there are not enough observations to accurately estimate the model parameters: in a similar situation there is a greater risk of <em class="ltx_emph ltx_font_italic">overfitting</em>, as the model may excessively adapt to noise in the data rather than capturing the true underlying patterns.”</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S1.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" id="subsubsec-113-choosing-the-optimal-model">
<span class="ltx_tag ltx_tag_subsubsection">1.1.3 </span>Choosing the Optimal Model</h4>

<div id="S1.SS1.SSS3.p1" class="ltx_para">
<p class="ltx_p">The model containing all of the predictors will always have the smallest RSS and the largest <math id="S1.SS1.SSS3.p1.m1" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>, since these quantities are related to the training error. We wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.
Therefore, RSS and <math id="S1.SS1.SSS3.p1.m2" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> are not suitable for selecting the best model among a collection of models with di erent numbers of predictors.</p>
</div>
<section id="S1.SS1.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Estimating test error: two approaches</h5>

<div id="S1.SS1.SSS3.Px1.p1" class="ltx_para">
<ol id="S1.I7" class="ltx_enumerate">
<li id="S1.I7.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I7.i1.p1" class="ltx_para">
<p class="ltx_p">We can indirectly estimate test error by making an <em class="ltx_emph ltx_font_italic">adjustment</em> to the training error to account for the bias due to overfitting.</p>
<ul id="S1.I7.i1.I1" class="ltx_itemize">
<li id="S1.I7.i1.I0.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I7.i1.I0.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I7.i1.I0.i1.p1" class="ltx_para">
<p class="ltx_p"><math id="S1.I7.i1.I0.i1.p1.m1" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math>, AIC, BIC, and Adjusted <math id="S1.I7.i1.I0.i1.p1.m2" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> techniques <em class="ltx_emph ltx_font_italic">adjust</em> the training error for the model size and can be used to select among a set of models with different numbers of variables.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S1.I7.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I7.i2.p1" class="ltx_para">
<p class="ltx_p">We can <em class="ltx_emph ltx_font_italic">directly</em> estimate the test error, using either a <em class="ltx_emph ltx_font_italic">validation set approach</em> or a <em class="ltx_emph ltx_font_italic">cross-validation approach</em></p>
<ul id="S1.I7.i2.I1" class="ltx_itemize">
<li id="S1.I7.i2.I0.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I7.i2.I0.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I7.i2.I0.i1.p1" class="ltx_para">
<p class="ltx_p">We compute the validation set error or the cross-validation error for each model <math id="S1.I7.i2.I0.i1.p1.m1" class="ltx_Math" alttext="M_{k}" display="inline"><msub><mi>M</mi><mi>k</mi></msub></math> under consideration, and then select the <math id="S1.I7.i2.I0.i1.p1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> for which the resulting estimated test error is smallest.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="S1.SS1.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">(1) Mallow’s <math id="S1.SS1.SSS3.Px2.m1" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math>
</h5>

<div id="S1.SS1.SSS3.Px2.p1" class="ltx_para">
<table id="S1.Ex2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex2.m1" class="ltx_Math" alttext="C_{p}=\frac{1}{n}\left(\text{RSS}+2d\hat{\sigma}^{2}\right)" display="block"><mrow><msub><mi>C</mi><mi>p</mi></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>⁢</mo><mrow><mo>(</mo><mrow><mtext>RSS</mtext><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><mi>d</mi><mo>⁢</mo><msup><mover accent="true"><mi>σ</mi><mo stretchy="false">^</mo></mover><mn>2</mn></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S1.SS1.SSS3.Px2.p1.m1" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> is the total number of parameters used and <math id="S1.SS1.SSS3.Px2.p1.m2" class="ltx_Math" alttext="\hat{\sigma}^{2}" display="inline"><msup><mover accent="true"><mi>σ</mi><mo stretchy="false">^</mo></mover><mn>2</mn></msup></math> is an estimate of the variance of the error <math id="S1.SS1.SSS3.Px2.p1.m3" class="ltx_Math" alttext="\epsilon" display="inline"><mi>ϵ</mi></math> associated with each response measurement.</p>
</div>
</section>
<section id="S1.SS1.SSS3.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">(1) AIC</h5>

<div id="S1.SS1.SSS3.Px3.p1" class="ltx_para">
<table id="S1.Ex3" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex3.m1" class="ltx_Math" alttext="\text{AIC}=-2\log(L)+2d" display="block"><mrow><mtext>AIC</mtext><mo>=</mo><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>⁢</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>⁢</mo><mi>d</mi></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S1.SS1.SSS3.Px3.p1.m1" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> is the maximized value of the likelihood function for the estimated model.

<br class="ltx_break">The AIC criterion is defined for a large class of models fit by maximum likelihood.</p>
</div>
<div id="S1.SS1.SSS3.Px3.p2" class="ltx_para">
<p class="ltx_p">In the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and <math id="S1.SS1.SSS3.Px3.p2.m1" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math> and <span class="ltx_text ltx_markedasmath">AIC</span> are equivalent.</p>
</div>
</section>
<section id="S1.SS1.SSS3.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">(1) BIC</h5>

<div id="S1.SS1.SSS3.Px4.p1" class="ltx_para">
<table id="S1.Ex4" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex4.m1" class="ltx_Math" alttext="\text{BIC}=\frac{1}{n}\left(\text{RSS}+\log(n)d\hat{\sigma}^{2}\right)" display="block"><mrow><mtext>BIC</mtext><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>⁢</mo><mrow><mo>(</mo><mrow><mtext>RSS</mtext><mo>+</mo><mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⁢</mo><mi>d</mi><mo>⁢</mo><msup><mover accent="true"><mi>σ</mi><mo stretchy="false">^</mo></mover><mn>2</mn></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Like <math id="S1.SS1.SSS3.Px4.p1.m1" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math>, the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.

<br class="ltx_break">Notice that BIC replaces the <math id="S1.SS1.SSS3.Px4.p1.m2" class="ltx_Math" alttext="2d\hat{\sigma}^{2}" display="inline"><mrow><mn>2</mn><mo>⁢</mo><mi>d</mi><mo>⁢</mo><msup><mover accent="true"><mi>σ</mi><mo stretchy="false">^</mo></mover><mn>2</mn></msup></mrow></math> used by <math id="S1.SS1.SSS3.Px4.p1.m3" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math> with a <math id="S1.SS1.SSS3.Px4.p1.m4" class="ltx_Math" alttext="\log(n)d\hat{\sigma}^{2}" display="inline"><mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>⁢</mo><mi>d</mi><mo>⁢</mo><msup><mover accent="true"><mi>σ</mi><mo stretchy="false">^</mo></mover><mn>2</mn></msup></mrow></math> term, where <math id="S1.SS1.SSS3.Px4.p1.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> is the number of observations.

<br class="ltx_break">Since <math id="S1.SS1.SSS3.Px4.p1.m6" class="ltx_Math" alttext="\log(n)&gt;2" display="inline"><mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mn>2</mn></mrow></math> for any <math id="S1.SS1.SSS3.Px4.p1.m7" class="ltx_Math" alttext="n&gt;7" display="inline"><mrow><mi>n</mi><mo>&gt;</mo><mn>7</mn></mrow></math>, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than <math id="S1.SS1.SSS3.Px4.p1.m8" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math>.</p>
</div>
</section>
<section id="S1.SS1.SSS3.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">(1) Adjusted <math id="S1.SS1.SSS3.Px5.m1" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>
</h5>

<div id="S1.SS1.SSS3.Px5.p1" class="ltx_para">
<table id="S1.Ex5" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex5.m1" class="ltx_Math" alttext="\text{Adjusted }R^{2}=1-\frac{\text{RSS}/(n-d-1)}{\text{TSS}/(n-1)}" display="block"><mrow><mrow><mtext>Adjusted </mtext><mo>⁢</mo><msup><mi>R</mi><mn>2</mn></msup></mrow><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mtext>RSS</mtext><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>-</mo><mi>d</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mtext>TSS</mtext><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where TSS is the total sum of squares.

<br class="ltx_break">This is the adjusted <math id="S1.SS1.SSS3.Px5.p1.m1" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> statistic for a least squares model with <math id="S1.SS1.SSS3.Px5.p1.m2" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> variables.

<br class="ltx_break">A large value of adjusted <math id="S1.SS1.SSS3.Px5.p1.m3" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> indicates a model with a small test error.

<br class="ltx_break">Maximizing the adjusted <math id="S1.SS1.SSS3.Px5.p1.m4" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> is equivalent to minimizing <math id="S1.SS1.SSS3.Px5.p1.m5" class="ltx_Math" alttext="\frac{\text{RSS}}{(n-d-1)}" display="inline"><mfrac><mtext>RSS</mtext><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>-</mo><mi>d</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mfrac></math>. While RSS always decreases as the number of variables in the model increases, <math id="S1.SS1.SSS3.Px5.p1.m6" class="ltx_Math" alttext="\frac{\text{RSS}}{(n-d-1)}" display="inline"><mfrac><mtext>RSS</mtext><mrow><mo stretchy="false">(</mo><mrow><mi>n</mi><mo>-</mo><mi>d</mi><mo>-</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mfrac></math> may increase or decrease, due to the presence of <math id="S1.SS1.SSS3.Px5.p1.m7" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> in the denominator.

<br class="ltx_break">Unlike the <math id="S1.SS1.SSS3.Px5.p1.m8" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> statistic, the adjusted <math id="S1.SS1.SSS3.Px5.p1.m9" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> statistic pays a price for the inclusion of unnecessary variables in the model.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="images/fig2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering" width="379" height="146" alt=", BIC, and adjusted ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><math id="S1.F2.m3" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math>, BIC, and adjusted <math id="S1.F2.m4" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> for the best model of each size produced by best subset selection on the Credit data set.</figcaption>
</figure>
</section>
<section id="S1.SS1.SSS3.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">(2) Validation and Cross-Validation</h5>

<div id="S1.SS1.SSS3.Px6.p1" class="ltx_para">
<p class="ltx_p">
<br class="ltx_break">Each of the procedures returns a sequence of models <math id="S1.SS1.SSS3.Px6.p1.m1" class="ltx_Math" alttext="M_{k}" display="inline"><msub><mi>M</mi><mi>k</mi></msub></math> indexed by model size <math id="S1.SS1.SSS3.Px6.p1.m2" class="ltx_Math" alttext="k=0,1,2,\ldots" display="inline"><mrow><mi>k</mi><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></math>. Our job here is to select <math id="S1.SS1.SSS3.Px6.p1.m3" class="ltx_Math" alttext="\hat{k}" display="inline"><mover accent="true"><mi>k</mi><mo stretchy="false">^</mo></mover></math>. Once selected, we will return model <math id="S1.SS1.SSS3.Px6.p1.m4" class="ltx_Math" alttext="M_{\hat{k}}" display="inline"><msub><mi>M</mi><mover accent="true"><mi>k</mi><mo stretchy="false">^</mo></mover></msub></math>.

<br class="ltx_break">This procedure has an advantage relative to AIC, BIC, <math id="S1.SS1.SSS3.Px6.p1.m5" class="ltx_Math" alttext="C_{p}" display="inline"><msub><mi>C</mi><mi>p</mi></msub></math>, and adjusted <math id="S1.SS1.SSS3.Px6.p1.m6" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>, in that it provides a direct estimate of the test error, and doesn’t require an estimate of the error variance <math id="S1.SS1.SSS3.Px6.p1.m7" class="ltx_Math" alttext="\sigma^{2}" display="inline"><msup><mi>σ</mi><mn>2</mn></msup></math>.

<br class="ltx_break">It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom or hard to esttimate the error variance <math id="S1.SS1.SSS3.Px6.p1.m8" class="ltx_Math" alttext="\sigma^{2}" display="inline"><msup><mi>σ</mi><mn>2</mn></msup></math>.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="images/fig3.png" id="S1.F3.g1" class="ltx_graphics ltx_centering" width="271" height="101" alt="All three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors. In this case, the validation and cross-validation methods both result in a six-variable model.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>All three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors. In this case, the validation and cross-validation methods both result in a six-variable model.</figcaption>
</figure>
<div id="S1.SS1.SSS3.Px6.p2" class="ltx_para">
<ul id="S1.I8" class="ltx_itemize">
<li id="S1.I8.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I8.i1.p1" class="ltx_para">
<p class="ltx_p">The validation errors were randomly calculated by selecting three-quarters of the observations as the training set, and the remainder as the validation set;</p>
</div>
</li>
<li id="S1.I8.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I8.i2.p1" class="ltx_para">
<p class="ltx_p">The cross-validation errors were computed using <math id="S1.I8.i2.p1.m1" class="ltx_Math" alttext="k=10" display="inline"><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow></math> folds;</p>
</div>
</li>
<li id="S1.I8.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I8.i3.p1" class="ltx_para">
<p class="ltx_p">In this setting, we can select a model using the <em class="ltx_emph ltx_font_italic">one-standard-error rule</em>: we first calculate the standard error of the estimated test MSE for each model size (number of predictors), and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" id="subsec-12-shrinkage-methods">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Shrinkage Methods</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p class="ltx_p">Regularization methods are techniques used in machine learning and statistics to improve a model’s generalization ability, which is its capacity to make accurate predictions on new, unseen data. These methods add penalties or constraints to the training process of the model, discouraging the model from fitting too closely to the training data (overfitting).

<br class="ltx_break">The subset selection methods use least squares to fit a linear model that contains a subset of the predictors; as an alternative, we can fit a model containing all <math id="S1.SS2.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors and constrain, regularize, or shrink the coefficient estimates towards zero.

<br class="ltx_break">It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance.</p>
</div>
<section id="S1.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" id="subsubsec-121-ridge-regression">
<span class="ltx_tag ltx_tag_subsubsection">1.2.1 </span>Ridge Regression</h4>

<div id="S1.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p">Recall that the least squares fitting procedure estimates <math id="S1.SS2.SSS1.p1.m1" class="ltx_Math" alttext="\beta_{0},\beta_{1},\ldots,\beta_{p}" display="inline"><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>,</mo><msub><mi>β</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>β</mi><mi>p</mi></msub></mrow></math> using the values that minimize:</p>
<table id="S1.Ex6" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex6.m1" class="ltx_Math" alttext="\text{RSS}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij}%
\right)^{2}" display="block"><mrow><mtext>RSS</mtext><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><msub><mi>β</mi><mn>0</mn></msub><mo>-</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">In contrast, the ridge regression coefficient estimates <math id="S1.SS2.SSS1.p1.m2" class="ltx_Math" alttext="\hat{\beta}^{R}" display="inline"><msup><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>R</mi></msup></math> are the values that minimize:</p>
<table id="S1.Ex7" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex7.m1" class="ltx_Math" alttext="\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}+%
\lambda\sum_{j=1}^{p}\beta_{j}^{2}=\text{RSS}+\lambda\sum_{j=1}^{p}\beta_{j}^{2}" display="block"><mrow><mrow><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><msub><mi>β</mi><mn>0</mn></msub><mo>-</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>λ</mi><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msubsup><mi>β</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mrow></mrow><mo>=</mo><mrow><mtext>RSS</mtext><mo>+</mo><mrow><mi>λ</mi><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msubsup><mi>β</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <math id="S1.SS2.SSS1.p1.m3" class="ltx_Math" alttext="\lambda\geq 0" display="inline"><mrow><mi>λ</mi><mo>≥</mo><mn>0</mn></mrow></math> is a tuning parameter, to be determined separately.</p>
</div>
<div id="S1.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p">As with least squares, ridge regression seeks coeffcient estimates that fit the data well, by making the RSS small.

<br class="ltx_break">However, the second term, <math id="S1.SS2.SSS1.p2.m1" class="ltx_Math" alttext="\lambda\sum_{j=1}^{p}\beta_{j}^{2}" display="inline"><mrow><mi>λ</mi><mo>⁢</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><msubsup><mi>β</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mrow></math>, called a <em class="ltx_emph ltx_font_italic">shrinkage penalty</em>, is small when <math id="S1.SS2.SSS1.p2.m2" class="ltx_Math" alttext="\beta_{1},\ldots,\beta_{p}" display="inline"><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>β</mi><mi>p</mi></msub></mrow></math> are close to zero, and so it has the effect of <em class="ltx_emph ltx_font_italic">shrinking</em> the estimates of <math id="S1.SS2.SSS1.p2.m3" class="ltx_Math" alttext="\beta_{j}" display="inline"><msub><mi>β</mi><mi>j</mi></msub></math> towards zero.</p>
<ul id="S1.I9" class="ltx_itemize">
<li id="S1.I9.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I9.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I9.i1.p1" class="ltx_para">
<p class="ltx_p">The tuning parameter <math id="S1.I9.i1.p1.m1" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> serves to control the relative impact of these two terms on the regression coefficient estimates;</p>
</div>
</li>
<li id="S1.I9.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I9.i2.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I9.i2.p1" class="ltx_para">
<p class="ltx_p">Ridge regression adds a penalty to the regression coefficients to reduce <em class="ltx_emph ltx_font_italic">overfitting</em>;</p>
</div>
</li>
<li id="S1.I9.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I9.i3.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I9.i3.p1" class="ltx_para">
<p class="ltx_p">Selecting a good value for <math id="S1.I9.i3.p1.m1" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> is critical; cross-validation is used for this.</p>
<ul id="S1.I9.i3.I1" class="ltx_itemize">
<li id="S1.I9.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="S1.I9.i3.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Increasing <math id="S1.I9.i3.I1.i1.p1.m1" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> reduces the complexity of the model (lower variance) but increases the bias (prediction error); for very large <math id="S1.I9.i3.I1.i1.p1.m2" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math>, all coefficients can be reduced close to zero, and the model may become too simple, failing to adequately capture the data.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<figure id="S1.F4" class="ltx_figure"><img src="images/fig4.png" id="S1.F4.g1" class="ltx_graphics ltx_centering" width="271" height="112" alt="In the ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>In the <em class="ltx_emph ltx_font_italic">left-hand panel</em>, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of <math id="S1.F4.m4" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math>. The <em class="ltx_emph ltx_font_italic">right-hand panel</em> displays the same ridge coefficient estimates as the left-hand panel, but we display <math id="S1.F4.m5" class="ltx_Math" alttext="\|\hat{\beta}_{\lambda}^{R}\|_{2}/\|\hat{\beta}\|_{2}" display="inline"><mrow><msub><mrow><mo>∥</mo><msubsup><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>λ</mi><mi>R</mi></msubsup><mo>∥</mo></mrow><mn>2</mn></msub><mo>/</mo><msub><mrow><mo>∥</mo><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mo>∥</mo></mrow><mn>2</mn></msub></mrow></math>, where <math id="S1.F4.m6" class="ltx_Math" alttext="\hat{\beta}" display="inline"><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover></math> denotes the vector of least squares coefficient estimates.</figcaption>
</figure>
<div id="S1.SS2.SSS1.p3" class="ltx_para">
<ul id="S1.I10" class="ltx_itemize">
<li id="S1.I10.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I10.i1.p1" class="ltx_para">
<p class="ltx_p">The notation <math id="S1.I10.i1.p1.m1" class="ltx_Math" alttext="\|\beta\|_{2}" display="inline"><msub><mrow><mo>∥</mo><mi>β</mi><mo>∥</mo></mrow><mn>2</mn></msub></math> denotes the <math id="S1.I10.i1.p1.m2" class="ltx_Math" alttext="\ell^{2}" display="inline"><msup><mi mathvariant="normal">ℓ</mi><mn>2</mn></msup></math> norm (pronounced ell 2) of a vector, and is defined as <math id="S1.I10.i1.p1.m3" class="ltx_Math" alttext="\|\beta\|_{2}=\sqrt{\sum_{j=1}^{p}\beta_{j}^{2}}" display="inline"><mrow><msub><mrow><mo>∥</mo><mi>β</mi><mo>∥</mo></mrow><mn>2</mn></msub><mo>=</mo><msqrt><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><msubsup><mi>β</mi><mi>j</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></math>.
</p>
</div>
</li>
<li id="S1.I10.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I10.i2.p1" class="ltx_para">
<p class="ltx_p"><math id="S1.I10.i2.p1.m1" class="ltx_Math" alttext="\|\hat{\beta}_{\lambda}^{R}\|_{2}/\|\hat{\beta}\|_{2}" display="inline"><mrow><msub><mrow><mo>∥</mo><msubsup><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>λ</mi><mi>R</mi></msubsup><mo>∥</mo></mrow><mn>2</mn></msub><mo>/</mo><msub><mrow><mo>∥</mo><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mo>∥</mo></mrow><mn>2</mn></msub></mrow></math> formula represents the ratio between the <math id="S1.I10.i2.p1.m2" class="ltx_Math" alttext="\ell_{2}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></math> norm of the ridge regression coefficients (<math id="S1.I10.i2.p1.m3" class="ltx_Math" alttext="\hat{\beta}_{\lambda}^{R}" display="inline"><msubsup><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>λ</mi><mi>R</mi></msubsup></math>) and the <math id="S1.I10.i2.p1.m4" class="ltx_Math" alttext="\ell_{2}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></math> norm of the ordinary least squares (OLS) regression coefficients (<math id="S1.I10.i2.p1.m5" class="ltx_Math" alttext="\hat{\beta}" display="inline"><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover></math>).</p>
</div>
</li>
<li id="S1.I10.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I10.i3.p1" class="ltx_para">
<p class="ltx_p"><math id="S1.I10.i3.p1.m1" class="ltx_Math" alttext="\hat{\beta}_{\lambda}^{R}" display="inline"><msubsup><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>λ</mi><mi>R</mi></msubsup></math>: represents the vector of estimated ridge regression coefficients for a given regularization parameter value <math id="S1.I10.i3.p1.m2" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math></p>
</div>
</li>
<li id="S1.I10.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I10.i4.p1" class="ltx_para">
<p class="ltx_p"><math id="S1.I10.i4.p1.m1" class="ltx_Math" alttext="\hat{\beta}" display="inline"><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover></math>: represents the vector of ordinary least squares (OLS) regression coefficients, i.e., the coefficients estimated without regularization</p>
</div>
</li>
<li id="S1.I10.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I10.i5.p1" class="ltx_para">
<p class="ltx_p"><math id="S1.I10.i5.p1.m1" class="ltx_Math" alttext="\lVert\cdot\rVert_{2}" display="inline"><msub><mrow><mo fence="true" stretchy="false">∥</mo><mo>⋅</mo><mo fence="true" stretchy="false">∥</mo></mrow><mn>2</mn></msub></math>: indicates the <math id="S1.I10.i5.p1.m2" class="ltx_Math" alttext="\ell_{2}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></math> norm of a vector, which is defined as the square root of the sum of squares of its elements. In other words, <math id="S1.I10.i5.p1.m3" class="ltx_Math" alttext="\lVert\cdot\rVert_{2}" display="inline"><msub><mrow><mo fence="true" stretchy="false">∥</mo><mo>⋅</mo><mo fence="true" stretchy="false">∥</mo></mrow><mn>2</mn></msub></math> calculates the ”length” of the vector in Euclidean space.</p>
</div>
</li>
<li id="S1.I10.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I10.i6.p1" class="ltx_para">
<p class="ltx_p">Therefore, the ratio <math id="S1.I10.i6.p1.m1" class="ltx_Math" alttext="\lVert\hat{\beta}_{\lambda}^{R}\rVert_{2}/\lVert\hat{\beta}\rVert_{2}" display="inline"><mrow><msub><mrow><mo fence="true" stretchy="false">∥</mo><msubsup><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>λ</mi><mi>R</mi></msubsup><mo fence="true" stretchy="false">∥</mo></mrow><mn>2</mn></msub><mo>/</mo><msub><mrow><mo fence="true" stretchy="false">∥</mo><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mo fence="true" stretchy="false">∥</mo></mrow><mn>2</mn></msub></mrow></math> measures how much the ridge regression coefficients deviate from the ordinary least squares regression coefficients, normalized by the ”length” of the ordinary least squares regression coefficients.

<br class="ltx_break">This ratio can provide information about the relative magnitude of the coefficients estimated by <em class="ltx_emph ltx_font_italic">ridge regression</em> compared to those estimated by <em class="ltx_emph ltx_font_italic">ordinary least squares regression</em>. A value greater than 1 indicates that the coefficients estimated by ridge regression are, on average, larger than the coefficients of the ordinary least squares regression, while a value less than 1 indicates the opposite.</p>
</div>
</li>
</ul>
<p class="ltx_p">The standard least squares coefficient estimates are <em class="ltx_emph ltx_font_italic">scale equivariant</em>: multiplying <math id="S1.SS2.SSS1.p3.m1" class="ltx_Math" alttext="X_{j}" display="inline"><msub><mi>X</mi><mi>j</mi></msub></math> by a constant <math id="S1.SS2.SSS1.p3.m2" class="ltx_Math" alttext="c" display="inline"><mi>c</mi></math> simply leads to a scaling of the least squares coefficient estimates by a factor of <math id="S1.SS2.SSS1.p3.m3" class="ltx_Math" alttext="1/c" display="inline"><mrow><mn>1</mn><mo>/</mo><mi>c</mi></mrow></math>. In other words, regardless of how the <math id="S1.SS2.SSS1.p3.m4" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math>th predictor is scaled, <math id="S1.SS2.SSS1.p3.m5" class="ltx_Math" alttext="X_{j}\hat{\beta}_{j}" display="inline"><mrow><msub><mi>X</mi><mi>j</mi></msub><mo>⁢</mo><msub><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>j</mi></msub></mrow></math> will remain the same.

<br class="ltx_break">In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.

<br class="ltx_break">Therefore, it is best to apply ridge regression after <em class="ltx_emph ltx_font_italic">standardizing the predictors</em>, using the formula</p>
<table id="S1.Ex8" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex8.m1" class="ltx_Math" alttext="\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\bar{x}_{j%
})^{2}}}" display="block"><mrow><msub><mover accent="true"><mi>x</mi><mo stretchy="false">~</mo></mover><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>⁢</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>-</mo><msub><mover accent="true"><mi>x</mi><mo stretchy="false">¯</mo></mover><mi>j</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></msqrt></mfrac></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<figure id="S1.F5" class="ltx_figure"><img src="images/fig5.png" id="S1.F5.g1" class="ltx_graphics ltx_centering" width="271" height="103" alt="Squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of <math id="S1.F5.m3" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> and <math id="S1.F5.m4" class="ltx_Math" alttext="\lVert\hat{\beta}_{\lambda}^{R}\rVert_{2}/\lVert\hat{\beta}\rVert_{2}" display="inline"><mrow><msub><mrow><mo fence="true" stretchy="false">∥</mo><msubsup><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>λ</mi><mi>R</mi></msubsup><mo fence="true" stretchy="false">∥</mo></mrow><mn>2</mn></msub><mo>/</mo><msub><mrow><mo fence="true" stretchy="false">∥</mo><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mo fence="true" stretchy="false">∥</mo></mrow><mn>2</mn></msub></mrow></math>. The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest.</figcaption>
</figure>
<div id="S1.SS2.SSS1.p4" class="ltx_para">
<ul id="S1.I11" class="ltx_itemize">
<li id="S1.I11.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I11.i1.p1" class="ltx_para">
<p class="ltx_p">The first plot illustrates how the Mean Squared Error (MSE) varies with the regularization parameter <math id="S1.I11.i1.p1.m1" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> in Ridge regression. This provides an indication of how the model responds to changes in <math id="S1.I11.i1.p1.m2" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math>.</p>
</div>
</li>
<li id="S1.I11.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">@itemi</span> 
<div id="S1.I11.i2.p1" class="ltx_para">
<p class="ltx_p">The second plot shows how the MSE varies with the ratio of the squared norm of the Ridge regression coefficient vector to the squared norm of the simple linear regression coefficient vector. This gives an insight into how much the Ridge regression solution deviates from the simple linear regression solution.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" id="subsubsec-122-the-lasso">
<span class="ltx_tag ltx_tag_subsubsection">1.2.2 </span>The Lasso</h4>

<div id="S1.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p">Ridge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all <math id="S1.SS2.SSS2.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors in the final model.

<br class="ltx_break">The Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, denoted by <math id="S1.SS2.SSS2.p1.m2" class="ltx_Math" alttext="\hat{\beta}_{\lambda}^{L}" display="inline"><msubsup><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mi>λ</mi><mi>L</mi></msubsup></math>, minimize the quantity</p>
<table id="S1.Ex9" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.Ex9.m1" class="ltx_Math" alttext="\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}+%
\lambda\sum_{j=1}^{p}|\beta_{j}|=\text{RSS}+\lambda\sum_{j=1}^{p}|\beta_{j}|" display="block"><mrow><mrow><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>-</mo><msub><mi>β</mi><mn>0</mn></msub><mo>-</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>λ</mi><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><mo stretchy="false">|</mo><msub><mi>β</mi><mi>j</mi></msub><mo stretchy="false">|</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mtext>RSS</mtext><mo>+</mo><mrow><mi>λ</mi><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><mo stretchy="false">|</mo><msub><mi>β</mi><mi>j</mi></msub><mo stretchy="false">|</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">In statistical parlance, the lasso uses an <math id="S1.SS2.SSS2.p1.m3" class="ltx_Math" alttext="\ell_{1}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub></math> (pronounced ”ell 1”) penalty instead of an <math id="S1.SS2.SSS2.p1.m4" class="ltx_Math" alttext="\ell_{2}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></math> penalty. The <math id="S1.SS2.SSS2.p1.m5" class="ltx_Math" alttext="\ell_{1}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub></math> norm of a coefficient vector <math id="S1.SS2.SSS2.p1.m6" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math> is given by <math id="S1.SS2.SSS2.p1.m7" class="ltx_Math" alttext="\|\beta\|_{1}=\sum|\beta_{j}|" display="inline"><mrow><msub><mrow><mo>∥</mo><mi>β</mi><mo>∥</mo></mrow><mn>1</mn></msub><mo>=</mo><mrow><mo largeop="true" symmetric="true">∑</mo><mrow><mo stretchy="false">|</mo><msub><mi>β</mi><mi>j</mi></msub><mo stretchy="false">|</mo></mrow></mrow></mrow></math>.

<br class="ltx_break">As with ridge regression, the lasso shrinks the coefficient estimates towards zero.

<br class="ltx_break">However, in the case of the lasso, the <math id="S1.SS2.SSS2.p1.m8" class="ltx_Math" alttext="\ell_{1}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub></math> penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter <math id="S1.SS2.SSS2.p1.m9" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> is sufficiently large.</p>
<ul id="S1.I12" class="ltx_itemize">
<li id="S1.I12.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I12.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I12.i1.p1" class="ltx_para">
<p class="ltx_p">much like best subset selection, the lasso performs <em class="ltx_emph ltx_font_italic">variable selection</em>. We say that the lasso yields <em class="ltx_emph ltx_font_italic">sparse models</em> — that is, models that involve only a subset of the variables.</p>
</div>
</li>
</ul>
<p class="ltx_p">As in ridge regression, selecting a good value of <math id="S1.SS2.SSS2.p1.m10" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> for the lasso is critical; cross-validation is again the method of choice.</p>
</div>
<figure id="S1.F6" class="ltx_figure"><span class="ltx_inline-para ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<span id="S1.F6.p1" class="ltx_para ltx_align_center"><img src="images/fig6.png" id="S1.F6.p1.g1" class="ltx_graphics" width="541" height="197" alt=": Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set. ">
</span></span><span class="ltx_inline-para ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<span id="S1.F6.p2" class="ltx_para ltx_align_center"><img src="images/fig7.png" id="S1.F6.p2.g1" class="ltx_graphics" width="541" height="195" alt=": Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set. ">
</span></span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><em class="ltx_emph ltx_font_italic">Left</em>: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set. <em class="ltx_emph ltx_font_italic">Right</em>: Comparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed); both are plotted against their <math id="S1.F6.m2" class="ltx_Math" alttext="R^{2}" display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest. Right image only two predictors are related to the response.</figcaption>
</figure>
<div id="S1.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">These two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.

<br class="ltx_break">In general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors. However, the number of predictors that is related to the response is never known a priori for real data sets.

<br class="ltx_break">A technique such as <em class="ltx_emph ltx_font_italic">cross-validation</em> can be used in order to determine which approach is better on a particular data set.</p>
</div>
</section>
<section id="S1.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" id="subsubsec-123-selecting-the-tuning-parameter-for-ridge-regression-and-lasso">
<span class="ltx_tag ltx_tag_subsubsection">1.2.3 </span>Selecting the Tuning Parameter for Ridge Regression and Lasso</h4>

<div id="S1.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p">We require a method selecting a value for the tuning parameter <math id="S1.SS2.SSS3.p1.m1" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> or equivalently, the value of the constraint <math id="S1.SS2.SSS3.p1.m2" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>. <em class="ltx_emph ltx_font_italic">Cross-validation</em> provides a simple way to tackle this problem:</p>
<ol id="S1.I13" class="ltx_enumerate">
<li id="S1.I13.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I13.i1.p1" class="ltx_para">
<p class="ltx_p">We choose a grid of <math id="S1.I13.i1.p1.m1" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> values, and compute the <em class="ltx_emph ltx_font_italic">cross-validation error rate</em> for each value of <math id="S1.I13.i1.p1.m2" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math>;</p>
</div>
</li>
<li id="S1.I13.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I13.i2.p1" class="ltx_para">
<p class="ltx_p">We select the tuning parameter value for which the cross-validation error is smallest;</p>
</div>
</li>
<li id="S1.I13.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I13.i3.p1" class="ltx_para">
<p class="ltx_p">The model is re-fit using all of the available observations and the selected value of the tuning parameter.</p>
</div>
</li>
</ol>
</div>
<figure id="S1.F7" class="ltx_figure"><img src="images/fig8.png" id="S1.F7.g1" class="ltx_graphics ltx_centering" width="271" height="98" alt=": cross-validation errors that result from applying ridge regression to the Credit data set with various values of ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><em class="ltx_emph ltx_font_italic">Left</em>: cross-validation errors that result from applying ridge regression to the Credit data set with various values of <math id="S1.F7.m4" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math>. <em class="ltx_emph ltx_font_italic">Right</em>: the coefficient estimates as a function of <math id="S1.F7.m5" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math>. The vertical dashed lines indicates the value of <math id="S1.F7.m6" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> selected by cross-validation.</figcaption>
</figure>
<figure id="S1.F8" class="ltx_figure"><img src="images/fig9.png" id="S1.F8.g1" class="ltx_graphics ltx_centering" width="271" height="99" alt=": ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set. ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><em class="ltx_emph ltx_font_italic">Left</em>: ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set. <em class="ltx_emph ltx_font_italic">Right</em>: the corresponding lasso coefficient estimates are displayed. The vertical dashed lines indicate the lasso fit for which the cross-validation error is smallest.</figcaption>
</figure>
</section>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" id="subsec-13-dimension-reduction-methods">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Dimension Reduction Methods</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p class="ltx_p">The methods that we have discussed so far have involved <em class="ltx_emph ltx_font_italic">fitting linear regression models</em>, via least squares or a shrunken approach, using the original predictors, <math id="S1.SS3.p1.m1" class="ltx_Math" alttext="X_{1},X_{2},\cdot,X_{p}" display="inline"><mrow><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><mo>⋅</mo><mo>,</mo><msub><mi>X</mi><mi>p</mi></msub></mrow></math>.

<br class="ltx_break">We now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p class="ltx_p">Let <math id="S1.SS3.p2.m1" class="ltx_Math" alttext="Z_{1},Z_{2},\ldots,Z_{M}" display="inline"><mrow><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>Z</mi><mi>M</mi></msub></mrow></math> represent <math id="S1.SS3.p2.m2" class="ltx_Math" alttext="M&lt;p" display="inline"><mrow><mi>M</mi><mo>&lt;</mo><mi>p</mi></mrow></math> linear combinations of our original <math id="S1.SS3.p2.m3" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors. That is,</p>
<table id="S1.E1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E1.m1" class="ltx_Math" alttext="Z_{m}=\sum_{j=1}^{p}\phi_{mj}X_{j}\quad\text{for some constants}\quad\phi_{m1}%
,\ldots,\phi_{mp}." display="block"><mrow><mrow><msub><mi>Z</mi><mi>m</mi></msub><mo>=</mo><mrow><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>ϕ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>⁢</mo><msub><mi>X</mi><mi>j</mi></msub></mrow></mrow><mo mathvariant="italic" separator="true"> </mo><mtext>for some constants</mtext><mo mathvariant="italic" separator="true"> </mo><msub><mi>ϕ</mi><mrow><mi>m</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>ϕ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>p</mi></mrow></msub></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr>
</table>
<p class="ltx_p">We can then fit the linear regression model,</p>
<table id="S1.E2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E2.m1" class="ltx_Math" alttext="y_{i}=\theta_{0}+\sum_{m=1}^{M}\theta_{m}z_{im}+\epsilon_{i},\quad i=1,\ldots,n," display="block"><mrow><mrow><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>+</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>θ</mi><mi>m</mi></msub><mo>⁢</mo><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>m</mi></mrow></msub></mrow></mrow><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub></mrow></mrow><mo rspace="12.5pt">,</mo><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr>
</table>
<p class="ltx_p">using ordinary least squares.

<br class="ltx_break">Note that in model (2) the regression coefficients are given by <math id="S1.SS3.p2.m4" class="ltx_Math" alttext="\theta_{0},\theta_{1},\ldots,\theta_{M}" display="inline"><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>θ</mi><mi>M</mi></msub></mrow></math>. If the constants <math id="S1.SS3.p2.m5" class="ltx_Math" alttext="\phi_{m1},\ldots,\phi_{mp}" display="inline"><mrow><msub><mi>ϕ</mi><mrow><mi>m</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>ϕ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>p</mi></mrow></msub></mrow></math> are chosen wisely, then such dimension reduction approaches can often outperform ordinary least squares (OLS) regression.</p>
</div>
<div id="S1.SS3.p3" class="ltx_para">
<p class="ltx_p">Notice that from definition (1),</p>
<table id="S1.E3" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E3.m1" class="ltx_Math" alttext="\sum_{m=1}^{M}\theta_{m}z_{im}=\sum_{m=1}^{M}\theta_{mj}\sum_{j=1}^{p}\phi_{mj%
}x_{ij}=\sum_{j=1}^{p}\sum_{m=1}^{M}\theta_{m}\phi_{mj}x_{ij}=\sum_{j=1}^{p}%
\beta_{j}x_{ij}" display="block"><mrow><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>θ</mi><mi>m</mi></msub><mo>⁢</mo><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>m</mi></mrow></msub></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>θ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>⁢</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>ϕ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>⁢</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>θ</mi><mi>m</mi></msub><mo>⁢</mo><msub><mi>ϕ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>⁢</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr>
</table>
<p class="ltx_p">where</p>
<table id="S1.E4" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S1.E4.m1" class="ltx_Math" alttext="\beta_{j}=\sum_{m=1}^{M}\theta_{m}\phi_{mj}." display="block"><mrow><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><msub><mi>θ</mi><mi>m</mi></msub><mo>⁢</mo><msub><mi>ϕ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr>
</table>
<p class="ltx_p">Hence, model (2) can be thought of as a special case of the original linear regression model.

<br class="ltx_break">Dimension reduction serves to constrain the estimated <math id="S1.SS3.p3.m1" class="ltx_Math" alttext="\beta_{j}" display="inline"><msub><mi>β</mi><mi>j</mi></msub></math> coefficients, since now they must take the form (3). This approach can help in achieving a favorable bias-variance tradeoff.</p>
</div>
<section id="S1.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" id="subsubsec-131-principal-components-regression">
<span class="ltx_tag ltx_tag_subsubsection">1.3.1 </span>Principal Components Regression</h4>

<div id="S1.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p">Here we apply principal components analysis (PCA) to define the linear combinations of the predictors for use in our regression.

<br class="ltx_break">Hence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation.</p>
</div>
<figure id="S1.F9" class="ltx_figure"><img src="images/fig10.png" id="S1.F9.g1" class="ltx_graphics ltx_centering" width="271" height="92" alt=": The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments. ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><em class="ltx_emph ltx_font_italic">Left</em>: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments. <em class="ltx_emph ltx_font_italic">Right</em>: The left-hand panel has been rotated so that the first principal component lies on the x-axis.</figcaption>
</figure>
<figure id="S1.F10" class="ltx_figure"><img src="images/fig11.png" id="S1.F10.g1" class="ltx_graphics ltx_centering" width="271" height="99" alt="PCR was applied to two simulated data sets. The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>PCR was applied to two simulated data sets. The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively.</figcaption>
</figure>
<figure id="S1.F11" class="ltx_figure"><img src="images/fig12.png" id="S1.F11.g1" class="ltx_graphics ltx_centering" width="271" height="110" alt=": PCR standardized coefficient estimates on the Credit data set for different values of ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><em class="ltx_emph ltx_font_italic">Left</em>: PCR standardized coefficient estimates on the Credit data set for different values of <math id="S1.F11.m3" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>. <em class="ltx_emph ltx_font_italic">Right</em>: The 10-fold cross validation MSE obtained using PCR, as a function of <math id="S1.F11.m4" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>.</figcaption>
</figure>
</section>
<section id="S1.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" id="subsubsec-132-partial-least-squares">
<span class="ltx_tag ltx_tag_subsubsection">1.3.2 </span>Partial Least Squares</h4>

<div id="S1.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p">PCR identifies linear combinations, or directions, that best represent the predictors <math id="S1.SS3.SSS2.p1.m1" class="ltx_Math" alttext="X_{1},X_{2},\cdot,X_{p}" display="inline"><mrow><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><mo>⋅</mo><mo>,</mo><msub><mi>X</mi><mi>p</mi></msub></mrow></math>. These directions are identified in an <em class="ltx_emph ltx_font_italic">unsupervised way</em>, since the response Y is not used to help determine the principal component directions.</p>
<ul id="S1.I14" class="ltx_itemize">
<li id="S1.I14.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I14.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I14.i1.p1" class="ltx_para">
<p class="ltx_p">the response does not supervise the identification of the principal components</p>
</div>
</li>
</ul>
<p class="ltx_p">Consequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.</p>
</div>
<div id="S1.SS3.SSS2.p2" class="ltx_para">
<p class="ltx_p">Like PCR, PLS is a dimension reduction method, which first identifies a new set of features <math id="S1.SS3.SSS2.p2.m1" class="ltx_Math" alttext="Z_{1},Z_{2},\cdot,Z_{M}" display="inline"><mrow><msub><mi>Z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Z</mi><mn>2</mn></msub><mo>,</mo><mo>⋅</mo><mo>,</mo><msub><mi>Z</mi><mi>M</mi></msub></mrow></math> that are linear combinations of the original features, and then fits a linear model via OLS using these <math id="S1.SS3.SSS2.p2.m2" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> new features.

<br class="ltx_break">But unlike PCR, PLS identifies these new features in a <em class="ltx_emph ltx_font_italic">supervised way</em></p>
<ul id="S1.I15" class="ltx_itemize">
<li id="S1.I15.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I15.i1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math></span> 
<div id="S1.I15.i1.p1" class="ltx_para">
<p class="ltx_p">it makes use of the response <math id="S1.I15.i1.p1.m1" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math> in order to identify new features that not only approximate the old features well, but also that are related to the response.</p>
</div>
</li>
</ul>
<p class="ltx_p">Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.</p>
</div>
<div id="S1.SS3.SSS2.p3" class="ltx_para">
<p class="ltx_p">After standardizing the <math id="S1.SS3.SSS2.p3.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> predictors, PLS computes the first direction <math id="S1.SS3.SSS2.p3.m2" class="ltx_Math" alttext="Z_{1}" display="inline"><msub><mi>Z</mi><mn>1</mn></msub></math> by setting each <math id="S1.SS3.SSS2.p3.m3" class="ltx_Math" alttext="\phi_{1j}" display="inline"><msub><mi>ϕ</mi><mrow><mn>1</mn><mo>⁢</mo><mi>j</mi></mrow></msub></math> in (1) equal to the coefficient from the simple linear regression of <math id="S1.SS3.SSS2.p3.m4" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math> onto <math id="S1.SS3.SSS2.p3.m5" class="ltx_Math" alttext="X_{j}" display="inline"><msub><mi>X</mi><mi>j</mi></msub></math>.
One can show that this coefficient is proportional to the correlation between <math id="S1.SS3.SSS2.p3.m6" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math> and <math id="S1.SS3.SSS2.p3.m7" class="ltx_Math" alttext="X_{j}" display="inline"><msub><mi>X</mi><mi>j</mi></msub></math>.
Hence, in computing <math id="S1.SS3.SSS2.p3.m8" class="ltx_Math" alttext="Z_{1}=\sum_{j=1}^{p}\phi_{1j}X_{j}" display="inline"><mrow><msub><mi>Z</mi><mn>1</mn></msub><mo>=</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup><mrow><msub><mi>ϕ</mi><mrow><mn>1</mn><mo>⁢</mo><mi>j</mi></mrow></msub><mo>⁢</mo><msub><mi>X</mi><mi>j</mi></msub></mrow></mrow></mrow></math>, PLS places the highest weight on the variables that are most strongly related to the response.</p>
</div>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Dec 15 22:42:48 2025 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
