\documentclass{article}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}


\begin{document}
\section{Subset selection, Regularization and Dimension Reduction}
We consider some approaches for extending the linear model framework: we generalize the linear model in order to accommodate non-linear, but still additive, relationships. 
\\Despite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance. We'll discuss some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures. \emph{Why consider alternatives to least squares?}
\begin{itemize}[label=-]
\item \emph{Prediction Accuracy}: especially when $p>n$, to control the variance.
\begin{itemize}[label=$\rightarrow$]
\item in the context of linear regression, when the number of predictors is larger than the number of observations, the least squares estimation may lead to \emph{overfitting}
\end{itemize}
\item \emph{Model Interpretability}: by removing irrelevant features --- that is, by setting the corresponding coefficient estimates to zero --- we can obtain a model that is more easily interpreted. 
\end{itemize}
We present some approaches for automatically performing feature selection.
Each class of methods serves a distinct purpose in regularization, offering different ways to address the trade-off between model complexity and overfitting. 

\paragraph{Three classes of methods}
\begin{itemize}
\item \textbf{Subset Selection} explicitly chooses a subset of features; we identify a subset of the $p$ predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.
\item \textbf{Shrinkage}: reduce the impact of features; we fit a model involving all $p$ predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as \emph{regularization}) has the effect of reducing variance and can also perform variable selection.
\item \textbf{Dimension Reduction} techniques transform the feature space to reduce dimensionality; we project the $p$ predictors into a $M$-dimensional subspace, where $M<p$. This is achieved by computing $M$ different linear combinations, or projections, of the variables. Then these $M$ projections are used as predictors to fit a linear regression model by least squares.
\end{itemize}


\subsection{Subset Selection}
We present \emph{best subset} and \emph{stepwise model} selection procedures

\subsubsection{Best Subset Selection}
\begin{enumerate}
\item Let $M_0$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.
\item For $k = 1, 2, \ldots, p$:
\begin{enumerate}
\item [2.1]Fit all  \( \binom{p}{k}\) models that contain exactly $k$ predictors.
\begin{itemize}[label=$\rightarrow$]
\item The binomial coefficient \( \binom{p}{k} \) is a mathematical formula that calculates the \emph{number of ways to choose \( k \) distinct elements} from a \emph{set of \( p \) elements}. It is read as "p choose k" and is calculated as follows:
\[\binom{p}{k} = \frac{p!}{k!(p-k)!}\]
\end{itemize}
\item [2.2]Pick the best among these  \( \binom{p}{k}\) models, and call it $M_k$. Here, best is defined as having the smallest RSS, or equivalently largest $R^2$.
\end{enumerate}
\item Select a single best model from among $M_0, M_1, \ldots, M_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.
\end{enumerate}

\begin{figure}[htp]
\centering
\includegraphics[width=0.7\textwidth]{fig1}
\caption{For each possible model containing a subset of the ten predictors in the Credit data set, the RSS and $R^2$ are displayed. The red frontier tracks the best model for a given number of predictors, according to RSS and $R^2$.}
\end{figure}

\noindent Although we have discussed best subset selection in the context of least squares regression, the same principles can be applied to other types of models, such as \emph{logistic regression}. In these cases, the deviance, which is negative two times the maximized log-likelihood, serves a similar role to the residual sum of squares (RSS) in a wider range of models.


\subsubsection{Stepwise Selection}
\emph{Best subset selection} may also suffer from statistical problems when $p$ is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data. Thus an enormous search space can lead to overfitting and high variance of the coefficient estimates.
\\For both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection.

\paragraph{Forward Stepwise Selection} begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model. 

\begin{enumerate}
\item Let $M_0$ denote the null model, which contains no predictors.
\item For $k = 0, 1, \ldots, p-1$:
\begin{enumerate}
\item[2.1] Consider all $p-k$ models that augment the predictors in $M_k$ with one additional predictor.
\item[2.2] Choose the best among these $p-k$ models, and call it $M_{k+1}$. Here, best is defined as having the smallest RSS or highest $R^2$.
\end{enumerate}
\item Select a single best model from among $M_0, M_1, \ldots, M_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.
\end{enumerate}
It is not guaranteed to find the best possible model out of all $2^p$ models containing subsets of the $p$ predictors.
\begin{table}
\centering
\begin{tabular}{c|ll}
\hline
Variables & Best subset & Forward stepwise \\
\hline
One & rating & rating \\
Two & rating, income & rating, income \\
Three & rating, income, student & rating, income, student \\
Four & cards, income, student, limit & rating, income, student, limit \\
\hline
\end{tabular}
\caption{The first four selected models for best subset selection and forward stepwise selection on the Credit data set. The first three models are identical but the fourth models differ.}
\end{table}

\paragraph{Backward Stepwise Selection} like forward stepwise selection provides an efficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all $p$ predictors, and then iteratively removes the least useful predictor, one-at-a-time.
\begin{enumerate}
\item Let $M_p$ denote the full model, which contains all $p$ predictors.
\item For $k = p, p-1, \ldots, 1$:
\begin{enumerate}
\item[2.1] Consider all $k$ models that contain all but one of the predictors in $M_k$, for a total of $k - 1$ predictors.
\item[2.2] Choose the best among these $k$ models, and call it $M_{k-1}$. Here, best is defined as having the smallest RSS or highest $R^2$.
\end{enumerate}
\item Select a single best model from among $M_0, M_1, \ldots, M_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.
\end{enumerate}

Like forward stepwise selection, the backward selection approach searches through only $1+p(p + 1)/2$ models, and so can be applied in settings where $p$ is too large to apply best subset selection.
Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the $p$ predictors.
Backward selection requires that the number of samples $n$ is larger than the number of variables $p$ (so that the full model can be fit). In contrast, forward stepwise can be used even when $n<p$, and so is the only viable subset method when $p$ is very large.
\begin{itemize}[label=$\rightarrow$]
\item If the number of samples is less than the number of predictors, the statistical test for predictor significance may not be reliable, as there are not enough observations to accurately estimate the model parameters: in a similar situation there is a greater risk of \emph{overfitting}, as the model may excessively adapt to noise in the data rather than capturing the true underlying patterns."
\end{itemize}

\subsubsection{Choosing the Optimal Model}
The model containing all of the predictors will always have the smallest RSS and the largest $R^2$, since these quantities are related to the training error. We wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.
Therefore, RSS and $R^2$ are not suitable for selecting the best model among a collection of models with di erent numbers of predictors.

\paragraph{Estimating test error: two approaches}
\begin{enumerate}
\item We can indirectly estimate test error by making an \emph{adjustment} to the training error to account for the bias due to overfitting.
\begin{itemize}[label=$\rightarrow$]
\item $C_p$, AIC, BIC, and Adjusted $R^2$ techniques \emph{adjust} the training error for the model size and can be used to select among a set of models with different numbers of variables.
\end{itemize}
\item We can \emph{directly} estimate the test error, using either a \emph{validation set approach} or a \emph{cross-validation approach}
\begin{itemize}[label=$\rightarrow$]
\item We compute the validation set error or the cross-validation error for each model $M_k$ under consideration, and then select the $k$ for which the resulting estimated test error is smallest.
\end{itemize}
\end{enumerate}

\paragraph{(1) Mallow's $C_p$}
\[C_p = \frac{1}{n} \left(\text{RSS} + 2d\hat{\sigma}^2 \right)\]
where $d$ is the total number of parameters used and $\hat{\sigma}^2$ is an estimate of the variance of the error $\epsilon$ associated with each response measurement.

\paragraph{(1) AIC}
\[\text{AIC}=-2\log(L)+2d\]
where $L$ is the maximized value of the likelihood function for the estimated model.
\\The AIC criterion is defined for a large class of models fit by maximum likelihood.

In the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and $\text{AIC}$ are equivalent.

\paragraph{(1) BIC}
\[\text{BIC} = \frac{1}{n} \left(\text{RSS} + \log(n)d\hat{\sigma}^2 \right)\]
Like $C_p$, the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.
\\Notice that BIC replaces the $2d\hat{\sigma}^2$ used by $C_p$ with a $\log(n)d\hat{\sigma}^2$ term, where $n$ is the number of observations. 
\\Since $\log(n) > 2$ for any $n > 7$, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than $C_p$.

\paragraph{(1) Adjusted $R^2$}
\[\text{Adjusted } R^2 = 1 - \frac{\text{RSS}/(n - d - 1)}{\text{TSS}/(n - 1)}\]
where TSS is the total sum of squares.
\\This is the adjusted $R^2$ statistic for a least squares model with $d$ variables.
\\A large value of adjusted $R^2$ indicates a model with a small test error. 
\\Maximizing the adjusted $R^2$ is equivalent to minimizing $\frac{\text{RSS}}{(n - d - 1)}$. While RSS always decreases as the number of variables in the model increases, $\frac{\text{RSS}}{(n - d - 1)}$ may increase or decrease, due to the presence of $d$ in the denominator.
\\Unlike the $R^2$ statistic, the adjusted $R^2$ statistic pays a price for the inclusion of unnecessary variables in the model.

\begin{figure}[htp]
\centering
\includegraphics[width=0.7\textwidth]{fig2}
\caption{$C_p$, BIC, and adjusted $R^2$ for the best model of each size produced by best subset selection on the Credit data set.}
\end{figure}

\paragraph{(2) Validation and Cross-Validation}\mbox{}\vspace{0.5em}\\
Each of the procedures returns a sequence of models $M_k$ indexed by model size $k=0,1,2,\ldots$. Our job here is to select $\hat{k}$. Once selected, we will return model $M_{\hat{k}}$.
\\This procedure has an advantage relative to AIC, BIC, $C_p$, and adjusted $R^2$, in that it provides a direct estimate of the test error, and doesn't require an estimate of the error variance \(\sigma^2\).
\\It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom or hard to esttimate the error variance \(\sigma^2\).
\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{fig3}
\caption{All three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors. In this case, the validation and cross-validation methods both result in a six-variable model.}
\end{figure}
\begin{itemize}[label=*]
\item The validation errors were randomly calculated by selecting three-quarters of the observations as the training set, and the remainder as the validation set;
\item The cross-validation errors were computed using $k=10$ folds;
\item In this setting, we can select a model using the \emph{one-standard-error rule}: we first calculate the standard error of the estimated test MSE for each model size (number of predictors), and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve.
\end{itemize}


\subsection{Shrinkage Methods}
Regularization methods are techniques used in machine learning and statistics to improve a model's generalization ability, which is its capacity to make accurate predictions on new, unseen data. These methods add penalties or constraints to the training process of the model, discouraging the model from fitting too closely to the training data (overfitting). 
\\The subset selection methods use least squares to fit a linear model that contains a subset of the predictors; as an alternative, we can fit a model containing all $p$ predictors and constrain, regularize, or shrink the coefficient estimates towards zero.
\\It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance.

\subsubsection{Ridge Regression}
Recall that the least squares fitting procedure estimates $\beta_0, \beta_1, \ldots, \beta_p$ using the values that minimize:
\[ \text{RSS} = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 \]
In contrast, the ridge regression coefficient estimates $\hat{\beta}^{R}$ are the values that minimize:
\[ \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 = \text{RSS} +  \lambda \sum_{j=1}^{p} \beta_j^2\]
where $\lambda \geq 0$ is a tuning parameter, to be determined separately.

As with least squares, ridge regression seeks coeffcient estimates that fit the data well, by making the RSS small.
\\However, the second term, \( \lambda \sum_{j=1}^{p} \beta_j^2\), called a \emph{shrinkage penalty}, is small when $\beta_1, \ldots, \beta_p$ are close to zero, and so it has the effect of \emph{shrinking} the estimates of $\beta_j$ towards zero.
\begin{itemize}[label=$\rightarrow$]
\item The tuning parameter $\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates;
\item Ridge regression adds a penalty to the regression coefficients to reduce \emph{overfitting};
\item Selecting a good value for $\lambda$ is critical; cross-validation is used for this.
\begin{itemize}[label=-]
\item Increasing \( \lambda \) reduces the complexity of the model (lower variance) but increases the bias (prediction error); for very large \( \lambda \), all coefficients can be reduced close to zero, and the model may become too simple, failing to adequately capture the data.
\end{itemize}
\end{itemize}

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{fig4}
\caption{In the \emph{left-hand panel}, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of $\lambda$. The \emph{right-hand panel} displays the same ridge coefficient estimates as the left-hand panel, but we display $\| \hat{\beta}_\lambda^R \|_2/\| \hat{\beta} \|_2$, where $\hat{\beta}$ denotes the vector of least squares coefficient estimates.}
\end{figure}
\begin{itemize}[label=*]
\item The notation $\|\beta\|_2$ denotes the $\ell^2$ norm (pronounced ell 2) of a vector, and is defined as $\| \beta \|_2 = \sqrt{\sum_{j=1}^{p} \beta_j^2}$.
\item $\| \hat{\beta}_\lambda^R \|_2/\| \hat{\beta} \|_2$ formula represents the ratio between the \( \ell_2 \) norm of the ridge regression coefficients (\( \hat{\beta}_{\lambda}^R \)) and the \(\ell_2\) norm of the ordinary least squares (OLS) regression coefficients (\(\hat{\beta}\)).
\item \( \hat{\beta}_{\lambda}^R \): represents the vector of estimated ridge regression coefficients for a given regularization parameter value \( \lambda \)
\item \( \hat{\beta} \): represents the vector of ordinary least squares (OLS) regression coefficients, i.e., the coefficients estimated without regularization
\item \( \lVert \cdot \rVert_2 \): indicates the \( \ell_2 \) norm of a vector, which is defined as the square root of the sum of squares of its elements. In other words, \( \lVert \cdot \rVert_2 \) calculates the "length" of the vector in Euclidean space.
\item Therefore, the ratio \( \lVert \hat{\beta}_{\lambda}^R \rVert_2/\lVert \hat{\beta} \rVert_2\) measures how much the ridge regression coefficients deviate from the ordinary least squares regression coefficients, normalized by the "length" of the ordinary least squares regression coefficients. 
\\This ratio can provide information about the relative magnitude of the coefficients estimated by \emph{ridge regression} compared to those estimated by \emph{ordinary least squares regression}. A value greater than 1 indicates that the coefficients estimated by ridge regression are, on average, larger than the coefficients of the ordinary least squares regression, while a value less than 1 indicates the opposite.
\end{itemize}
The standard least squares coefficient estimates are \emph{scale equivariant}: multiplying $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$. In other words, regardless of how the $j$th predictor is scaled, $X_j\hat{\beta}_j$ will remain the same.
\\In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.
\\Therefore, it is best to apply ridge regression after \emph{standardizing the predictors}, using the formula
\[\tilde{x}_{ij} = \frac{x_{ij}}{\sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_{ij} - \bar{x}_j)^2}}\]


\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{fig5}
\caption{Squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of $\lambda$ and \( \lVert \hat{\beta}_{\lambda}^R \rVert_2/\lVert \hat{\beta} \rVert_2\). The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest.}
\end{figure}

\begin{itemize}[label=*]
\item The first plot illustrates how the Mean Squared Error (MSE) varies with the regularization parameter \( \lambda \) in Ridge regression. This provides an indication of how the model responds to changes in \( \lambda \).
\item The second plot shows how the MSE varies with the ratio of the squared norm of the Ridge regression coefficient vector to the squared norm of the simple linear regression coefficient vector. This gives an insight into how much the Ridge regression solution deviates from the simple linear regression solution.
\end{itemize}

\subsubsection{The Lasso}
Ridge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all $p$ predictors in the final model.
\\The Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, denoted by $\hat{\beta}_\lambda^{L}$, minimize the quantity
\[ \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j| = \text{RSS} +  \lambda \sum_{j=1}^{p} |\beta_j|\]
In statistical parlance, the lasso uses an $\ell_1$ (pronounced "ell 1") penalty instead of an $\ell_2$ penalty. The $\ell_1$ norm of a coefficient vector $\beta$ is given by $\|\beta\|_1 = \sum |\beta_j|$.
\\As with ridge regression, the lasso shrinks the coefficient estimates towards zero.
\\However, in the case of the lasso, the $\ell_1$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\lambda$ is sufficiently large.
\begin{itemize}[label=$\rightarrow$]
\item much like best subset selection, the lasso performs \emph{variable selection}. We say that the lasso yields \emph{sparse models} --- that is, models that involve only a subset of the variables.
\end{itemize}
As in ridge regression, selecting a good value of $\lambda$ for the lasso is critical; cross-validation is again the method of choice.

\begin{figure}[htp]
\centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{fig6}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{fig7}
\end{minipage}
\caption{\emph{Left}: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set. \emph{Right}: Comparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed); both are plotted against their $R^2$ on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest. Right image only two predictors are related to the response.}
\end{figure}

\noindent These two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.
\\In general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors. However, the number of predictors that is related to the response is never known a priori for real data sets.
\\A technique such as \emph{cross-validation} can be used in order to determine which approach is better on a particular data set.


\subsubsection{Selecting the Tuning Parameter for Ridge Regression and Lasso}
We require a method selecting a value for the tuning parameter $\lambda$ or equivalently, the value of the constraint $s$. \emph{Cross-validation} provides a simple way to tackle this problem: 
\begin{enumerate}
\item We choose a grid of $\lambda$ values, and compute the \emph{cross-validation error rate} for each value of $\lambda$;
\item We select the tuning parameter value for which the cross-validation error is smallest;
\item The model is re-fit using all of the available observations and the selected value of the tuning parameter.
\end{enumerate}

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{fig8}
\caption{\emph{Left}: cross-validation errors that result from applying ridge regression to the Credit data set with various values of $\lambda$. \emph{Right}: the coefficient estimates as a function of $\lambda$. The vertical dashed lines indicates the value of $\lambda$ selected by cross-validation.}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{fig9}
\caption{\emph{Left}: ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set. \emph{Right}: the corresponding lasso coefficient estimates are displayed. The vertical dashed lines indicate the lasso fit for which the cross-validation error is smallest.}
\end{figure}


\subsection{Dimension Reduction Methods}
The methods that we have discussed so far have involved \emph{fitting linear regression models}, via least squares or a shrunken approach, using the original predictors, $X_1,X_2,\cdot,X_p$.
\\We now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods.

Let \(Z_1, Z_2, \ldots, Z_M\) represent \(M<p\) linear combinations of our original \( p\) predictors. That is,
\begin{equation}
Z_m = \sum_{j=1}^{p} \phi_{mj} X_j \quad \text{for some constants} \quad \phi_{m1}, \ldots, \phi_{mp}.
\end{equation}
We can then fit the linear regression model,
\begin{equation}
y_i = \theta_0 + \sum_{m=1}^{M} \theta_m z_{im} + \epsilon_i, \quad i = 1, \ldots, n,
\end{equation}
using ordinary least squares.
\\Note that in model (2) the regression coefficients are given by \( \theta_0, \theta_1, \ldots, \theta_M \). If the constants \(\phi_{m1}, \ldots, \phi_{mp}\) are chosen wisely, then such dimension reduction approaches can often outperform ordinary least squares (OLS) regression.

Notice that from definition (1),
\begin{equation}
\sum_{m=1}^{M} \theta_m z_{im} = \sum_{m=1}^{M} \theta_{mj} \sum_{j=1}^{p} \phi_{mj} x_{ij} = \sum_{j=1}^{p} \sum_{m=1}^{M} \theta_m \phi_{mj} x_{ij} = \sum_{j=1}^{p} \beta_j x_{ij}
\end{equation}
where
\begin{equation}
\beta_j = \sum_{m=1}^{M} \theta_m \phi_{mj}.
\end{equation}
Hence, model (2) can be thought of as a special case of the original linear regression model. 
\\Dimension reduction serves to constrain the estimated \(\beta_j\) coefficients, since now they must take the form (3). This approach can help in achieving a favorable bias-variance tradeoff.


\subsubsection{Principal Components Regression}
Here we apply principal components analysis (PCA) to define the linear combinations of the predictors for use in our regression.
\\Hence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation.

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{fig10}
\caption{\emph{Left}: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments. \emph{Right}: The left-hand panel has been rotated so that the first principal component lies on the x-axis.}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{fig11}
\caption{PCR was applied to two simulated data sets. The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively.}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{fig12}
\caption{\emph{Left}: PCR standardized coefficient estimates on the Credit data set for different values of $M$. \emph{Right}: The 10-fold cross validation MSE obtained using PCR, as a function of $M$.}
\end{figure}


\subsubsection{Partial Least Squares}
PCR identifies linear combinations, or directions, that best represent the predictors $X_1,X_2,\cdot,X_p$. These directions are identified in an \emph{unsupervised way}, since the response Y is not used to help determine the principal component directions.
\begin{itemize}[label=$\rightarrow$]
\item the response does not supervise the identification of the principal components
\end{itemize}
Consequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.

Like PCR, PLS is a dimension reduction method, which first identifies a new set of features $Z_1,Z_2,\cdot,Z_M$ that are linear combinations of the original features, and then fits a linear model via OLS using these $M$ new features.
\\But unlike PCR, PLS identifies these new features in a \emph{supervised way} 
\begin{itemize}[label=$\rightarrow$]
\item it makes use of the response $Y$ in order to identify new features that not only approximate the old features well, but also that are related to the response.
\end{itemize}
Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.

After standardizing the $p$ predictors, PLS computes the first direction $Z_1$ by setting each $\phi_{1j}$ in (1) equal to the coefficient from the simple linear regression of $Y$ onto $X_j$.
One can show that this coefficient is proportional to the correlation between $Y$ and $X_j$.
Hence, in computing $Z_1 = \sum_{j=1}^{p} \phi_{1j} X_j$, PLS places the highest weight on the variables that are most strongly related to the response.

\end{document}